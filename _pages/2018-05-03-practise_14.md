---
layout: page
title:  "Практика 14"
date:   2018-05-03 23:23:23 +0600
navigation: 14
---

# Практика 14

В этой лабе вы напишите многопоточное приложение с классическим "потребителем" и "поставщиком". 

## Scraper

Web scraping [wiki](https://en.wikipedia.org/wiki/Web_scraping) - это процесс скачивания и последующего разбора веб страниц. Ваша задача - написать свой web-scraper, который полностью скачает все гиперссылки (`<a href='ADDRESS'>` элементы) в рамках этого сайта (во вне выходить нельзя, иначе мы скачаем весь интернет). То есть вам не надо качать картинки и страницы, а только сохранять сами адреса. По своей сути мы сделаем ограниченную версию `wget`. Примеры использования: [тут](https://apple.stackexchange.com/a/100573) и [тут](https://unix.stackexchange.com/questions/293697/recursively-download-from-a-website).

Экспериментировать можете с сайтом [ru.wikipedia.org/wiki/Заглавная_страница](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D0%B0%D1%8F_%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86%D0%B0).

## Инструкции

Рекомендуется использовать библиотеку `BeautifulSoup4`. Русскоязычный [туториал](http://wiki.python.su/%D0%94%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D0%B8/BeautifulSoup), официальная [документация](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).  
Поставить пакет можно командой `pip install beautifulsoup4`.

### Как должно всё работать

1. Консольная утилита принимает как аргумент адрес сайта.

2. Программа скачивает к себе в память html страницу по этому адресу.

3. Программа разбирает html и ищет все `<a>` теги и извлекает `href` параметр.

4. По каждому href идем в 2 пункт.

## Однопоточная рекурсивная версия

Напишите в лоб.

## Многопоточная версия с очередями

ОГО!!

Теперь сравните скорость выполнения задачи.

## Дополнительные материалы

- [Advanced Web Scraping tutorial](http://sangaline.com/post/advanced-web-scraping-tutorial/) - интересная статья, в которой рассказывают с какими трудностями сталкиваются при web-скрейпинге.